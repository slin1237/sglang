name: PR Benchmark (SMG Components)

on:
  push:
    branches: [ main ]
    paths:
      - "sgl-model-gateway/**"
  pull_request:
    branches: [ main ]
    paths:
      - "sgl-model-gateway/**"
  workflow_dispatch:

concurrency:
  group: pr-benchmark-rust-${{ github.ref }}
  cancel-in-progress: true

env:
  RUSTC_WRAPPER: sccache
  SCCACHE_GHA_ENABLED: "true"

permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  # Quick check job that always runs on PRs
  benchmark-compile-check:
    name: Benchmark Compilation Check
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          bash scripts/ci/ci_install_rust.sh

      - name: Configure sccache
        uses: mozilla-actions/sccache-action@v0.0.9
        with:
          version: "v0.12.0"

      - name: Rust cache
        uses: Swatinem/rust-cache@v2
        with:
          workspaces: sgl-model-gateway
          # Share cache across all benchmark jobs
          shared-key: "rust-cache"
          # Save cache even on failure
          save-if: true
          cache-all-crates: true
          cache-on-failure: true

      - name: Check benchmarks compile
        run: |
          source "$HOME/.cargo/env"
          cd sgl-model-gateway/
          cargo check --benches

      - name: Show sccache stats
        if: always()
        run: sccache --show-stats

  # Full benchmark job that only runs with label or on main branch
  # Runs on self-hosted A10 nodes for consistent hardware (CPU-only benchmarks)
  benchmark-all:
    name: Model Gateway Benchmarks
    if: |
      github.repository == 'sgl-project/sglang' &&
      (github.event_name == 'push' ||
       github.event_name == 'workflow_dispatch' ||
       (contains(github.event.pull_request.labels.*.name, 'router-benchmark') &&
        contains(github.event.pull_request.labels.*.name, 'run-ci')))
    runs-on: 4-gpu-a10
    timeout-minutes: 60
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 100

      - name: Install dependencies
        run: |
          bash scripts/ci/ci_install_rust.sh

      - name: Configure sccache
        uses: mozilla-actions/sccache-action@v0.0.9
        with:
          version: "v0.12.0"

      - name: Rust cache
        uses: Swatinem/rust-cache@v2
        with:
          workspaces: sgl-model-gateway
          shared-key: "rust-cache"
          cache-all-crates: true
          cache-on-failure: true
          save-if: true

      - name: Build all benchmarks
        run: |
          source "$HOME/.cargo/env"
          cd sgl-model-gateway/
          mkdir -p benchmark_outputs
          # Build all benchmarks first to avoid parallel compilation conflicts
          cargo build --release --benches

      - name: Run all benchmarks in parallel
        run: |
          source "$HOME/.cargo/env"
          cd sgl-model-gateway/

          # Run all 3 benchmarks in parallel as background processes
          cargo bench --bench request_processing -- benchmark_summary --exact 2>&1 | tee benchmark_outputs/request_processing.txt &
          PID1=$!

          cargo bench --bench tokenizer_benchmark 2>&1 | tee benchmark_outputs/tokenizer.txt &
          PID2=$!

          cargo bench --bench tool_parser_benchmark 2>&1 | tee benchmark_outputs/tool_parser.txt &
          PID3=$!

          # Wait for all benchmarks to complete and capture exit codes
          EXIT1=0; wait $PID1 || EXIT1=$?
          EXIT2=0; wait $PID2 || EXIT2=$?
          EXIT3=0; wait $PID3 || EXIT3=$?

          echo "Benchmark exit codes: request_processing=$EXIT1, tokenizer=$EXIT2, tool_parser=$EXIT3"

          # Fail if any benchmark failed
          if [ $EXIT1 -ne 0 ] || [ $EXIT2 -ne 0 ] || [ $EXIT3 -ne 0 ]; then
            echo "One or more benchmarks failed"
            exit 1
          fi

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: |
            sgl-model-gateway/target/criterion/
            sgl-model-gateway/benchmark_outputs/
          retention-days: 30

      - name: Show sccache stats
        if: always()
        run: sccache --show-stats

  benchmark-summary:
    name: Benchmark Summary
    needs: [benchmark-all]
    if: always() && (github.repository == 'sgl-project/sglang' || github.event_name == 'pull_request')
    runs-on: ubuntu-latest
    steps:
      - name: Download all benchmark results
        uses: actions/download-artifact@v4
        with:
          pattern: 'benchmark-results-${{ github.sha }}'
          path: benchmark-results

      - name: Generate summary
        run: |
          echo "## ðŸš€ Model Gateway Benchmark Results" > summary.md
          echo "" >> summary.md

          RESULTS_DIR="benchmark-results/benchmark-results-${{ github.sha }}"

          # Request Processing Benchmark
          echo "### Request Processing" >> summary.md
          REQ_FILE="$RESULTS_DIR/benchmark_outputs/request_processing.txt"
          if [ -f "$REQ_FILE" ]; then
            echo "âœ… **Completed**" >> summary.md
            echo "" >> summary.md
            echo "<details>" >> summary.md
            echo "<summary>View Results</summary>" >> summary.md
            echo "" >> summary.md
            echo '```' >> summary.md
            grep -A 100 "SGLang Model Gateway\|Quick Performance\|Performance Insights" "$REQ_FILE" | head -50 >> summary.md || tail -60 "$REQ_FILE" >> summary.md
            echo '```' >> summary.md
            echo "</details>" >> summary.md
          else
            echo "âŒ Failed or skipped" >> summary.md
          fi
          echo "" >> summary.md

          # Tokenizer Benchmark
          echo "### Tokenizer" >> summary.md
          TOK_FILE="$RESULTS_DIR/benchmark_outputs/tokenizer.txt"
          if [ -f "$TOK_FILE" ]; then
            echo "âœ… **Completed**" >> summary.md
            echo "" >> summary.md
            echo "<details>" >> summary.md
            echo "<summary>View Results</summary>" >> summary.md
            echo "" >> summary.md
            echo '```' >> summary.md
            grep -A 200 "TOKENIZER BENCHMARK SUMMARY\|ENCODING THROUGHPUT" "$TOK_FILE" | head -100 >> summary.md || tail -100 "$TOK_FILE" >> summary.md
            echo '```' >> summary.md
            echo "</details>" >> summary.md
          else
            echo "âŒ Failed or skipped" >> summary.md
          fi
          echo "" >> summary.md

          # Tool Parser Benchmark
          echo "### Tool Parser" >> summary.md
          TOOL_FILE="$RESULTS_DIR/benchmark_outputs/tool_parser.txt"
          if [ -f "$TOOL_FILE" ]; then
            echo "âœ… **Completed**" >> summary.md
            echo "" >> summary.md
            echo "<details>" >> summary.md
            echo "<summary>View Results</summary>" >> summary.md
            echo "" >> summary.md
            echo '```' >> summary.md
            grep -A 200 "TOOL PARSER BENCHMARK SUMMARY\|REGISTRY OPERATIONS\|COMPLETE PARSING" "$TOOL_FILE" | head -100 >> summary.md || tail -100 "$TOOL_FILE" >> summary.md
            echo '```' >> summary.md
            echo "</details>" >> summary.md
          else
            echo "âŒ Failed or skipped" >> summary.md
          fi

          echo "" >> summary.md
          echo "---" >> summary.md
          echo "_Generated at $(date -u '+%Y-%m-%d %H:%M:%S UTC') on self-hosted A10 runner_" >> summary.md

          cat summary.md
          cat summary.md >> $GITHUB_STEP_SUMMARY

      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-summary-${{ github.sha }}
          path: summary.md
          retention-days: 30
